name: End-to-End Testing

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run weekly on Sundays at 3 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of E2E test to run'
        required: false
        default: 'all'
        type: choice
        options:
        - all
        - dependencies
        - discovery
        - pipeline
        - templates
        - cli
        - vivado
        - container
        - performance
        - fallbacks
      verbose:
        description: 'Enable verbose output'
        required: false
        default: false
        type: boolean
      no_cleanup:
        description: 'Preserve test artifacts'
        required: false
        default: false
        type: boolean

permissions:
  contents: read
  security-events: write
  actions: read
  pull-requests: write

env:
  # Environment variables for testing
  PCILEECH_ALLOW_MOCK_DATA: "true"
  PCILEECH_PRODUCTION_MODE: "false"
  PCILEECH_AUTO_INSTALL: "1"
  PYTHONPATH: "${{ github.workspace }}/src"

jobs:
  e2e-test:
    name: End-to-End Testing
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.9', '3.10', '3.11', '3.12']
        container-engine: ['docker', 'podman']
        include:
          # Test with full dependencies
          - python-version: '3.11'
            container-engine: 'podman'
            test-mode: 'full'
        exclude:
          # Skip some combinations to reduce CI time
          - python-version: '3.10'
            container-engine: 'podman'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          submodules: recursive

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends \
            build-essential \
            linux-headers-generic \
            pciutils \
            kmod \
            git \
            curl \
            jq

      - name: Install container engine
        if: matrix.container-engine == 'podman'
        run: |
          sudo apt-get install -y podman
          podman --version

      - name: Verify Docker availability
        if: matrix.container-engine == 'docker'
        run: |
          docker --version
          docker info

      - name: Create virtual environment
        run: |
          python -m venv venv
          source venv/bin/activate
          python -m pip install --upgrade pip setuptools wheel

      - name: Install minimal dependencies
        if: matrix.test-mode == 'minimal'
        run: |
          source venv/bin/activate
          pip install -r requirements.txt

      - name: Install full dependencies
        if: matrix.test-mode == 'full' || matrix.test-mode == ''
        run: |
          source venv/bin/activate
          pip install -r requirements.txt
          pip install -r requirements-test.txt
          pip install -r requirements-tui.txt || echo "TUI dependencies optional"

      - name: Install package in development mode
        run: |
          source venv/bin/activate
          pip install -e .

      - name: Build VFIO constants
        run: |
          source venv/bin/activate
          chmod +x build_vfio_constants.sh
          ./build_vfio_constants.sh || echo "VFIO constants build failed (expected in CI)"

      - name: Set up mock environment
        run: |
          # Create mock kernel modules directory
          sudo mkdir -p /lib/modules/$(uname -r)/kernel/vfio
          
          # Create mock VFIO devices
          sudo mkdir -p /dev/vfio
          sudo touch /dev/vfio/vfio
          sudo touch /dev/vfio/42
          
          # Set permissions
          sudo chmod 666 /dev/vfio/vfio /dev/vfio/42
          
          # Create mock module info
          echo "Mock VFIO modules loaded" | sudo tee /proc/modules > /dev/null || true

      - name: Run E2E tests (all)
        if: github.event.inputs.test_type == '' || github.event.inputs.test_type == 'all'
        run: |
          source venv/bin/activate
          python scripts/e2e_test_github_actions.py \
            ${{ github.event.inputs.no_cleanup == 'true' && '--no-cleanup' || '' }}

      - name: Run E2E tests (specific)
        if: github.event.inputs.test_type != '' && github.event.inputs.test_type != 'all'
        run: |
          source venv/bin/activate
          python scripts/e2e_test_github_actions.py \
            --test ${{ github.event.inputs.test_type }} \
            ${{ github.event.inputs.no_cleanup == 'true' && '--no-cleanup' || '' }}

      - name: Collect system information
        if: always()
        run: |
          echo "=== System Information ===" > system_info.txt
          echo "OS: $(lsb_release -a 2>/dev/null || cat /etc/os-release)" >> system_info.txt
          echo "Kernel: $(uname -a)" >> system_info.txt
          echo "Python: $(python --version)" >> system_info.txt
          echo "Available Memory: $(free -h)" >> system_info.txt
          echo "Available Disk: $(df -h /)" >> system_info.txt
          echo "Container Engine: ${{ matrix.container-engine }}" >> system_info.txt
          echo "Test Mode: ${{ matrix.test-mode }}" >> system_info.txt
          
          if command -v docker >/dev/null 2>&1; then
            echo "Docker Version: $(docker --version)" >> system_info.txt
          fi
          
          if command -v podman >/dev/null 2>&1; then
            echo "Podman Version: $(podman --version)" >> system_info.txt
          fi
          
          echo "=== Python Packages ===" >> system_info.txt
          source venv/bin/activate
          pip list >> system_info.txt || echo "Failed to list packages" >> system_info.txt

      - name: Upload test artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-artifacts-py${{ matrix.python-version }}-${{ matrix.container-engine }}
          path: |
            tests/e2e_temp/
            system_info.txt
            *.log
          if-no-files-found: warn
          retention-days: 7

      - name: Upload test reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-reports-py${{ matrix.python-version }}-${{ matrix.container-engine }}
          path: |
            tests/e2e_temp/**/e2e_test_*.json
          if-no-files-found: warn
          retention-days: 30

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: e2e-test
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install bandit safety

      - name: Run Bandit security scan
        run: |
          bandit -r src/ -f json -o bandit-report.json || true
          bandit -r src/ -f txt

      - name: Run Safety dependency scan
        run: |
          safety check --json --output safety-report.json || true
          safety check

      - name: Upload security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-scan-reports
          path: |
            bandit-report.json
            safety-report.json
          retention-days: 30

  performance-analysis:
    name: Performance Analysis
    runs-on: ubuntu-latest
    needs: e2e-test
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install psutil memory-profiler pytest-benchmark

      - name: Run performance tests
        run: |
          PCILEECH_ALLOW_MOCK_DATA=true python scripts/e2e_test_github_actions.py --test performance --verbose

      - name: Generate performance report
        run: |
          echo "=== Performance Analysis ===" > performance_report.txt
          echo "Timestamp: $(date -u)" >> performance_report.txt
          echo "Commit: ${{ github.sha }}" >> performance_report.txt
          echo "Branch: ${{ github.ref_name }}" >> performance_report.txt
          
          # Add performance test results
          if [ -f tests/e2e_temp/e2e_test_performance_report.json ]; then
            echo "=== Performance Test Results ===" >> performance_report.txt
            jq '.tests[] | select(.test_name == "Performance Benchmarks")' tests/e2e_temp/e2e_test_performance_report.json >> performance_report.txt
          fi

      - name: Upload performance reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-analysis-reports
          path: |
            performance_report.txt
            tests/e2e_temp/**/e2e_test_performance_report.json
          retention-days: 30

  container-test:
    name: Container Integration Test
    runs-on: ubuntu-latest
    needs: e2e-test
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build container image
        run: |
          docker build -t pcileech-fw-generator:ci-test -f Containerfile .

      - name: Test container functionality
        run: |
          # Test basic container functionality
          docker run --rm pcileech-fw-generator:ci-test python3 -c "import sys; print('Python:', sys.version)"
          
          # Test package imports
          docker run --rm pcileech-fw-generator:ci-test python3 -c "
          import sys
          sys.path.append('/app/src')
          try:
              from build import FirmwareBuilder
              print('✅ Core imports successful')
          except Exception as e:
              print('❌ Import failed:', e)
              sys.exit(1)
          "
          
          # Test CLI help
          docker run --rm pcileech-fw-generator:ci-test python3 /app/pcileech.py --help

      - name: Test container E2E
        run: |
          # Create test output directory
          mkdir -p /tmp/container-test-output
          
          # Run container-based E2E test with mock data
          docker run --rm \
            -e PCILEECH_ALLOW_MOCK_DATA=true \
            -e PCILEECH_PRODUCTION_MODE=false \
            -v /tmp/container-test-output:/app/output \
            pcileech-fw-generator:ci-test \
            python3 /app/scripts/e2e_test_github_actions.py --test dependencies --verbose

      - name: Upload container test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: container-test-results
          path: |
            /tmp/container-test-output/
          if-no-files-found: warn
          retention-days: 7

  integration-summary:
    name: Integration Test Summary
    runs-on: ubuntu-latest
    needs: [e2e-test, security-scan, performance-analysis, container-test]
    if: always()
    
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v5
        with:
          path: artifacts/

      - name: Generate summary report
        run: |
          echo "# E2E Integration Test Summary" > SUMMARY.md
          echo "" >> SUMMARY.md
          echo "**Workflow Run:** ${{ github.run_number }}" >> SUMMARY.md
          echo "**Commit:** ${{ github.sha }}" >> SUMMARY.md
          echo "**Branch:** ${{ github.ref_name }}" >> SUMMARY.md
          echo "**Triggered by:** ${{ github.event_name }}" >> SUMMARY.md
          echo "**Timestamp:** $(date -u)" >> SUMMARY.md
          echo "" >> SUMMARY.md
          
          echo "## Test Results" >> SUMMARY.md
          echo "" >> SUMMARY.md
          
          # Analyze E2E test results
          echo "### End-to-End Tests" >> SUMMARY.md
          for report in artifacts/e2e-test-reports-*/tests/e2e_temp/**/e2e_test_*.json; do
            if [ -f "$report" ]; then
              echo "Processing: $report" >> SUMMARY.md
              # Extract summary information (would need jq in real implementation)
            fi
          done
          
          # Check if any tests failed
          if [ "${{ needs.e2e-test.result }}" != "success" ]; then
            echo "❌ **E2E Tests:** FAILED" >> SUMMARY.md
          else
            echo "✅ **E2E Tests:** PASSED" >> SUMMARY.md
          fi
          
          if [ "${{ needs.security-scan.result }}" != "success" ] && [ "${{ needs.security-scan.result }}" != "skipped" ]; then
            echo "❌ **Security Scan:** FAILED" >> SUMMARY.md
          elif [ "${{ needs.security-scan.result }}" == "success" ]; then
            echo "✅ **Security Scan:** PASSED" >> SUMMARY.md
          else
            echo "⏸️ **Security Scan:** SKIPPED" >> SUMMARY.md
          fi
          
          if [ "${{ needs.performance-analysis.result }}" != "success" ] && [ "${{ needs.performance-analysis.result }}" != "skipped" ]; then
            echo "❌ **Performance Analysis:** FAILED" >> SUMMARY.md
          elif [ "${{ needs.performance-analysis.result }}" == "success" ]; then
            echo "✅ **Performance Analysis:** PASSED" >> SUMMARY.md
          else
            echo "⏸️ **Performance Analysis:** SKIPPED" >> SUMMARY.md
          fi
          
          if [ "${{ needs.container-test.result }}" != "success" ]; then
            echo "❌ **Container Test:** FAILED" >> SUMMARY.md
          else
            echo "✅ **Container Test:** PASSED" >> SUMMARY.md
          fi
          
          echo "" >> SUMMARY.md
          echo "## Artifacts" >> SUMMARY.md
          echo "" >> SUMMARY.md
          echo "- Test reports and logs available in workflow artifacts" >> SUMMARY.md
          echo "- Artifacts retained for 7-30 days depending on type" >> SUMMARY.md
          
          cat SUMMARY.md

      - name: Upload integration summary
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-summary
          path: SUMMARY.md
          retention-days: 90

      - name: Comment on PR (if applicable)
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('SUMMARY.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });
